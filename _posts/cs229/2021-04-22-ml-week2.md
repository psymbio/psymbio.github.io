---
layout: post
title: "Lecture 3: Locally Weighted & Logistic Regression"
tag: CS229
category: posts
order: 2
pagestyle: base
pagestyle2: post
---
Okay so now instead of just straight lines, we'll be dealing with curves and so the definition of $$h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 ...$$ changes from this to something like a qudratic line $$h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2^2$$ and if we don't want it to look like a parabola we could say $$h(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt[2]{x_2}$$

<iframe src="https://www.desmos.com/calculator/wswxzktjwn" width="100%" style="min-height:400px"></iframe>

Now feature design algorithms help to decide whether x should be squared or rooted or should the log of x be taken so as to fit the function.

## Locally Weighted Regression

What if the data isn't fit well by just a straight line? How do you find the features then? We use locally weighted regression.

Parmetric lerning algorithms: Fit a fixed set of parameters $$\theta_i$$ to the data.

Non-parametric learning algorithms: (locally weighted uses this) The amount of data/parameters you need to keep grows linearly with the size of data. (Not great if you have a large dataset becasue you need to keep all the data around loaded in the memory)

In linear regression: To evaluate the output at x, fit $$\theta$$ to minimize the cost function $$J(\theta_0, \theta_1) = \frac{1}{2m} (y^{(i)}-\theta^{T}(x^{(i)}))^2$$ and return $$\theta^{T}(x)$$

In a locally weighted regression you look at those value close to the value you want to predict and draw a line there.

<figure>
<img alt="regression" src="/images/2021/cs229/reg4.jpg" height="100%" width="100%">
<figcaption align="center">Normal linear regression vs. Locally weighted regression</figcaption>
</figure>

In locally weighted regression, fit $$\theta$$ to minimize $$\sum_{i=1}^{m} w^{i} (y^{(i)}-\theta^{T}(x^{(i)}))^2$$ where $$w^{i}$$ is a "weight" function.

A default choice of $$w^{i}$$ will be $$exp(\frac{-(x^{(i)}-x)^2}{2\tau^2})$$ and if $$|x^{(i)}-x|$$ is small then $$w^{(i)} \approx 1$$ and if $$|x^{(i)}-x|$$ is large then $$w^{(i)} \approx 0$$. So now if the term that you predit is far away from an $$x^{(i)}$$ then multiply the term with zero.

How do you choose the widh of the gaussain function? The parameter $$\tau$$ is the bandwidth parameter that lets you decide on how big the neighbourhood of the value to be predicted should be. Depending on $$\tau$$ we can choose a fatter or thinner bell shaped curve. The value of $$\tau$$ has an influence on whether the data is underfitted or overfitted.

## Probabilistic Interpretation

Why is the least square error taken? [24:00]

[Explanation](http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes1.pdf#page=10)