---
layout: post
title: "Lecture 3: Locally Weighted & Logistic Regression"
tag: CS229
category: posts
order: 2
---
Okay so now instead of just straight lines, we'll be dealing with curves and so the definition of $$h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 ...$$ changes from this to something like a qudratic line $$h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2^2$$ and if we don't want it to look like a parabola we could say $$h(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt[2]{x_2}$$

<iframe src="https://www.desmos.com/calculator/wswxzktjwn" width="100%" style="min-height:400px"></iframe>

Now feature design algorithms help to decide whether x should be squared or rooted or should the log of x be taken so as to fit the function.

## Locally Weighted Regression

What if the data isn't fit well by just a straight line? 